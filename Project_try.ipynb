{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1840d2e-bc04-4d79-9002-9af3cac2c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2547f59-9f2c-44ae-b79c-f56af71c7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88dd9129-ffd3-4a37-9779-fcdbb5476828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HF\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path, target_sr=16000):\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    audio = audio.set_frame_rate(target_sr).set_channels(1)\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    print(f\"Converted {mp3_path} to {wav_path} at {target_sr} Hz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c45e5d70-7b0c-4173-9b12-22605d97d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def preprocess_audio(input_dir, output_dir, sr=16000, n_mels=128):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for label in ['real', 'fake']:\n",
    "        input_class_dir = os.path.join(input_dir, label)\n",
    "        output_class_dir = os.path.join(output_dir, label)\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "        for file in os.listdir(input_class_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                y, sr = librosa.load(os.path.join(input_class_dir, file), sr=sr)\n",
    "                mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "                mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "                output_file = os.path.join(output_class_dir, file.replace('.wav', '.npy'))\n",
    "                np.save(output_file, mel_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6311c132-2230-4f8c-b57b-ba30271b5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedAudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, target_shape=(128, 300)):\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.target_shape = target_shape  # Fixed size for all spectrograms\n",
    "\n",
    "        for label in ['real', 'fake']:\n",
    "            class_dir = os.path.join(data_dir, label)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith('.npy'):\n",
    "                    self.file_paths.append(os.path.join(class_dir, filename))\n",
    "                    self.labels.append(0 if label == 'real' else 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spec = np.load(self.file_paths[idx])\n",
    "\n",
    "        # Get target shape\n",
    "        target_mels, target_frames = self.target_shape\n",
    "\n",
    "        # Padding or cropping logic\n",
    "        if spec.shape[1] < target_frames:\n",
    "            pad_width = target_frames - spec.shape[1]\n",
    "            spec = np.pad(spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            spec = spec[:, :target_frames]\n",
    "\n",
    "        # Convert to tensor and add channel dimension\n",
    "        spec_tensor = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return spec_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf75e604-70a5-40c6-9c35-8a812ebe505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class DeepFakeAudioDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepFakeAudioDetector, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7fabf98-b76e-4285-b53f-3ea09e598811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validate with Early Stopping & Checkpointing\n",
    "def train_model(data_dir, epochs=20, batch_size=8, lr=1e-4, patience=5):\n",
    "    dataset = PrecomputedAudioDataset(data_dir)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = DeepFakeAudioDetector().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate(model, val_loader, device, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Early Stopping & Checkpoint\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'C://HAMMAD/AI Data/DeepFake Audio/best_deepfake_detector.pth')\n",
    "            print(\"Saved new best model!\\n\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    # Plot final confusion matrix\n",
    "    plot_confusion_matrix(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9143b8a-1d30-4113-acce-c8ec3bfec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation with Loss & Accuracy\n",
    "def validate(model, val_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28d15553-44bb-4268-b196-c24272e07672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Plot\n",
    "def plot_confusion_matrix(model, val_loader, device):\n",
    "    model.load_state_dict(torch.load('best_deepfake_detector.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
    "            y_pred.extend(predictions)\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d38f2056-a752-489e-b610-4bde9f06e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HF\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\HF\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\HF\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.4453 | Val Loss: 0.3676 | Val Acc: 87.21%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 2 | Train Loss: 0.3018 | Val Loss: 0.2720 | Val Acc: 87.21%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 3 | Train Loss: 0.2490 | Val Loss: 0.2527 | Val Acc: 87.21%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 4 | Train Loss: 0.2154 | Val Loss: 0.2040 | Val Acc: 89.03%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 5 | Train Loss: 0.1928 | Val Loss: 0.2094 | Val Acc: 91.38%\n",
      "Patience: 1/5\n",
      "Epoch 6 | Train Loss: 0.1768 | Val Loss: 0.1842 | Val Acc: 92.69%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 7 | Train Loss: 0.1668 | Val Loss: 0.1649 | Val Acc: 90.60%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 8 | Train Loss: 0.1610 | Val Loss: 0.1549 | Val Acc: 91.64%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 9 | Train Loss: 0.1501 | Val Loss: 0.1490 | Val Acc: 95.30%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 10 | Train Loss: 0.1449 | Val Loss: 0.1494 | Val Acc: 92.95%\n",
      "Patience: 1/5\n",
      "Epoch 11 | Train Loss: 0.1344 | Val Loss: 0.1341 | Val Acc: 96.34%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 12 | Train Loss: 0.1317 | Val Loss: 0.1237 | Val Acc: 96.87%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 13 | Train Loss: 0.1291 | Val Loss: 0.1210 | Val Acc: 97.39%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 14 | Train Loss: 0.1182 | Val Loss: 0.1183 | Val Acc: 97.91%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 15 | Train Loss: 0.1298 | Val Loss: 0.1265 | Val Acc: 95.82%\n",
      "Patience: 1/5\n",
      "Epoch 16 | Train Loss: 0.1150 | Val Loss: 0.1068 | Val Acc: 97.65%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 17 | Train Loss: 0.1113 | Val Loss: 0.1112 | Val Acc: 98.43%\n",
      "Patience: 1/5\n",
      "Epoch 18 | Train Loss: 0.1080 | Val Loss: 0.1081 | Val Acc: 98.69%\n",
      "Patience: 2/5\n",
      "Epoch 19 | Train Loss: 0.1229 | Val Loss: 0.1062 | Val Acc: 98.17%\n",
      "Saved new best model!\n",
      "\n",
      "Epoch 20 | Train Loss: 0.1050 | Val Loss: 0.1107 | Val Acc: 94.78%\n",
      "Patience: 1/5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_deepfake_detector.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m preprocess_audio(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC://HAMMAD/AI Data/archive/for-original/for-original/training\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Dataset folder\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC://HAMMAD/AI Data/pectrogram deepfake\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Processed spectrogram folder\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Train with early stopping & checkpointing\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC://HAMMAD/AI Data/pectrogram deepfake\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(data_dir, epochs, batch_size, lr, patience)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Plot final confusion matrix\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m plot_confusion_matrix(model, val_loader, device)\n",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[1;34m(model, val_loader, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion_matrix\u001b[39m(model, val_loader, device):\n\u001b[1;32m----> 3\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_deepfake_detector.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      6\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_deepfake_detector.pth'"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_audio(\n",
    "        \"C://HAMMAD/AI Data/archive/for-original/for-original/training\",  # Dataset folder\n",
    "        \"C://HAMMAD/AI Data/pectrogram deepfake\"  # Processed spectrogram folder\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Step 2: Train with early stopping & checkpointing\n",
    "    train_model(\"C://HAMMAD/AI Data/pectrogram deepfake\", epochs=20, batch_size=8, lr=1e-4, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3e2213f-9b09-4b80-9dfa-4587603b411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path to audio file (.mp3 or .wav):  C://HAMMAD/AI Data/New folder/Ai audio check.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted mp3 to wav: C://HAMMAD/AI Data/New folder/Ai audio check_converted.wav\n",
      "Prediction: Audio file is 'FAKE'\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the model\n",
    "model = DeepFakeAudioDetector()\n",
    "\n",
    "# 2. Load the trained weights\n",
    "model.load_state_dict(torch.load('C://HAMMAD/AI Data/DeepFake Audio/best_deepfake_detector.pth'))\n",
    "\n",
    "# 3. Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def predict_single_file(file_path, model, target_sr=16000, target_shape=(128, 300)):\n",
    "    import os\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import librosa\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    if file_path.endswith('.mp3'):\n",
    "        y, sr = librosa.load(file_path, sr=target_sr)\n",
    "        wav_path = os.path.splitext(file_path)[0] + \"_converted.wav\"\n",
    "        sf.write(wav_path, y, sr)\n",
    "        file_path = wav_path\n",
    "        print(f\"Converted mp3 to wav: {file_path}\")\n",
    "\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    target_mels, target_frames = target_shape\n",
    "    if mel_spec_db.shape[1] < target_frames:\n",
    "        pad_width = target_frames - mel_spec_db.shape[1]\n",
    "        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel_spec_db = mel_spec_db[:, :target_frames]\n",
    "\n",
    "    input_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prediction = torch.argmax(output, dim=1).item()\n",
    "        label = \"REAL\" if prediction == 0 else \"FAKE\"\n",
    "        print(f\"Prediction: Audio file is '{label}'\")\n",
    "\n",
    "# 4. Ask user for input path\n",
    "file_path = input(\"Enter path to audio file (.mp3 or .wav): \")\n",
    "\n",
    "# 5. Predict!\n",
    "predict_single_file(file_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4ddd8-eb34-403d-ba44-1437eb6eeed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
